{
  "source_document": "SDK Comprehensive Development Guide_.md",
  "essential_facts": [
    "LM Studio is a local AI appliance with a desktop inference server and an SDK.",
    "The lmstudio-python SDK is a client library that communicates with the LM Studio server.",
    "The server manages hardware abstraction (CUDA, Vulkan), inference engine selection (e.g., llama.cpp, MLX), and resource allocation based on available VRAM/RAM.",
    "The Python SDK's API is auto-generated from the lmstudio-js (TypeScript) project's JSON schema, which defines the canonical API contract.",
    "The LM Studio GUI itself is a client built on the lmstudio-js SDK.",
    "LM Studio has a dual API strategy: a native WebSocket API and an OpenAI-compatible REST API.",
    "The native WebSocket API is used by the official SDKs for performance and structured data exchange, using the `msgspec` library.",
    "The OpenAI-compatible REST API is available at http://localhost:1234/v1.",
    "The OpenAI-compatible API enables integration with tools like LangChain, LlamaIndex, CodeGPT for VS Code, and JetBrains AI Assistant.",
    "Key OpenAI endpoints: /v1/chat/completions, /v1/embeddings, /v1/models.",
    "A beta Native REST API exists at /api/v0/... for richer metadata (e.g., model file paths).",
    "SDK Paradigms: Interactive Convenience API (import lmstudio as lms) and Scoped Resource API (with lms.Client() as client:).",
    "Convenience API uses `atexit` hooks for non-deterministic resource cleanup.",
    "Scoped Resource API is recommended for production code for deterministic resource cleanup.",
    "Requires Python 3.11+ (supports 3.12, 3.13) and LM Studio App v0.2.14+.",
    "Install SDK: pip install lmstudio-python (includes httpx, httpx-ws, msgspec dependencies).",
    "Install CLI: npx lmstudio install-cli",
    "CLI Bootstrap (macOS/Linux): ~/.lmstudio/bin/lms bootstrap",
    "CLI Bootstrap (Windows): cmd /c %USERPROFILE%/.lmstudio/bin/lms.exe bootstrap",
    "Server Config: Enable CORS for web development.",
    "Server Config: 'Just-In-Time Model Loading' loads models on first request.",
    "Server Config: 'Run LLM Server on Login' enables headless mode.",
    "Configure custom host/port: with lms.Client(base_url=...)",
    "List downloaded models: client.system.list_downloaded_models(model_type=\"llm\")",
    "List loaded models: client.llm.list_loaded()",
    "Get model info: model.get_info()",
    "Get model load config: model.get_load_config()",
    "Get-or-load a model: client.llm.model(\"<model_identifier>\")",
    "Load a new instance of a model: client.llm.load_new_instance()",
    "Load-Time Parameters (set at load): context_length, gpu_offload, rope_frequency_base.",
    "Inference-Time Parameters (set per request): temperature, max_tokens, stop_strings, response_format.",
    "Calling .model() with a config for an already-loaded model will ignore the new config.",
    "Unload a model manually: model.unload()",
    "Unload all models: lms unload --all (CLI)",
    "Auto-unload a model after inactivity (SDK v0.3.9+): client.llm.model(\"...\"), ttl=<seconds>)",
    "Key architecture: Separation of the stateless model handle from the stateful lms.Chat object.",
    "High-level chat method: model.respond(chat_history)",
    "Text completion method: model.complete(prompt)",
    "Enable streaming responses: stream=True (older alias: respond_stream).",
    "Prediction callbacks include: on_prompt_processing_progress, on_first_token, on_prediction_fragment.",
    "Cancel a streaming prediction: stream_object.cancel()",
    "Enforce structured JSON output: response_format={\"type\": \"pydantic\", \"class\": <PydanticModel>}",
    "Structured output uses grammar-constrained sampling (e.g., GBNF).",
    "Multimodal VLM interaction (SDK v1.1.0+): chat_history.add_user_message(text=\"...\", image_path=\"...\")",
    "Generate embeddings: client.embedding.model(\"...\").embed(<texts>)",
    "Server-side tokenization (SDK v1.2.0+): model.count_tokens(), model.tokenize()",
    "SDK-native agent loop: model.act(task, tools=[...])",
    "Tools for .act() are Python functions with type hints and descriptive docstrings.",
    ".act() callbacks include: on_message, on_round_* events.",
    "Default .act() behavior catches tool exceptions and passes them to the LLM; can be overridden with `handle_invalid_tool_request` callback.",
    "The .act() method supports concurrent tool calls via the `max_parallel_tool_calls` parameter.",
    "Model Context Protocol (MCP) is a separate GUI plugin system (LM Studio v0.3.17+), managed via mcp.json.",
    "MCP security feature: a 'tool call confirmation dialog' requires user permission in the GUI.",
    "The SDK has NO programmatic control over GUI RAG (GitHub Issue #69) or MCP servers (GitHub Issue #68).",
    "The OpenAI endpoint supports tool_choice with values \"none\", \"auto\", \"required\", but not forcing a specific function (GitHub Issue #670).",
    "The logit_bias parameter is currently ambiguous or buggy (GitHub Issue #87).",
    "Known issue: Parallel requests to the server can fail; use a client-side queue.",
    "Known issue: OpenAI-compatible libraries require a dummy API key (e.g., api_key=\"lm-studio\").",
    "The SDK does not expose a rich hierarchy of custom exception classes, instead propagating `httpx` exceptions.",
    "lms CLI command to list models: lms ls",
    "lms CLI command to download models: lms get <search_term>@<quantization>",
    "lms CLI command to load a model: lms load <model_key> --gpu max --context-length 4096",
    "lms CLI command to manage server: lms server start | stop | status",
    "lms CLI command to scaffold a project: lms create",
    "Recommended dev setup: virtual environment, tox, pdm.",
    "Example of Simple API Wrapper pattern: llm-lmstudio plugin.",
    "Example of SDK-Native Integration pattern: ComfyUI nodes for image-to-text.",
    "Example of Standalone App pattern: ChromaDB-Plugin-for-LM-Studio."
  ]
}
